{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PACW5MnobNuJ"
   },
   "source": [
    "<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">\n",
    "\n",
    "\n",
    "# Text generation using RNN/LSTM (Character-level)\n",
    "In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tdq3corNb22b"
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "<div>\n",
    "- <a href=\"#intro\">Introduction</a><br />\n",
    "- <a href=\"#arch\">Architectures</a><br />\n",
    "- <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a><br />\n",
    "- <a href=\"#build\">Building a LSTM with TensorFlow</a>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjk6jKSMbNuK"
   },
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x07SIHMbNuL"
   },
   "source": [
    "First, import the requiered libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPFEVJ75bNuL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "#from tensorflow.python.ops import rnn_cell\n",
    "#from tensorflow.python.ops import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN28hSr6bNuO"
   },
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sY_Z7U3fbNuP"
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btfca01hbNuR"
   },
   "source": [
    "### Parameters\n",
    "#### Batch, number_of_batch, batch_size and seq_length\n",
    "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
    "\n",
    "Lets assume the input is this sentence: '__here is an example__'. Then:\n",
    "- txt_length = 18  \n",
    "- seq_length = 3  \n",
    "- batch_size = 2  \n",
    "- number_of_batchs = 18/3*2 = 3\n",
    "- batch = array (['h','e','r'],['e',' ','i'])\n",
    "- sample Seq = 'her'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwJUzlwTbNuS"
   },
   "source": [
    "Ok, now, lets look at a real dataset, with real parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVBg_fWRbNuT"
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7qy5i-YbNuV"
   },
   "source": [
    "We download the input file, and print a part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11003,
     "status": "ok",
     "timestamp": 1561312272360,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "sNiPhNiybNuW",
    "outputId": "2521f5ce-720e-4314-e041-050a21f06e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-23 17:51:13 URL:https://public.boxcloud.com/d/1/b1!HR5tS5EMynfXCxx1pah_6vDVSlwwp0O6vLmePjqSUW73tw3_J4jx5W5tzBHKAOmHc0150QJpXGN6R62cC29DIaVGoPSg3yVcyPdGwKX12ZJO2RTo8nzXB8BR4F7jqgQibLV3Nbe_pasMrECuLRzQQfrhP8PAQcKdn1GoKbaJklqq1xGD9pmKn-GqW7_ju5jfqVsXLqeDit_pajirrN8M9ZBb6MOuM1IksX7a4bVaKHABb7TMRVnDcrpyJfQSAo8RANYAQIT444ZtZyc6IBINaLOZHhtFJbd80cdJEJ7XyujTBwWbG4cUfg4Jlvx4h_UWHAcxLfDpP8nkKau-_2QTeEeff61Bt7INKdn_wbKBYTLhJX1FlkuWrtP1niI2NLuC7WJMOwOvjI8Dmjy66qn0OxEmv5Bu6jy_PzMr_tvRTS9ROVunGe_XTCoxVJgQ2DluP4y6xnokV_76jErWnM2iOZNjouY-QgCNfwDuLOqvdW3q0ieqtuRifmHDaG2SZNARFTs9AkKIc3CnTuf_OYHW5zrMdPKxe5wlBwjJ2m36Z8EmAqvTeUtixS0pHtRdabOYpxtI82dF0Ij8c7rNDIzLk0JRBll-r9idBpq-K3v9-70B1n-Dl0VghwbDpYCRgpMit-xkG714aql85xKc5GsRZXAMmomv6qOkzza8l-o5hhSXK7zNuK9_YAIzlZoBdZ0QcImK_Ji2HHeOKf01YY1LfsZBxa0XlDFYlSCRVt3FHhLREAYJ8TW-P8H6ah4h0jy6V7fUE-PkQ3TW7TDxN76kJkbjLNOVCWtaUAjWmAHc8QeZBSWnaWVOUHxTof7xUSJ0fxv8AeDGM0qylskJZqW9V0q7mQ_GouIetecEgp4Fic0i3C43bqDMVasyjPTvVRFEC78PcJQ15iU09KAz8f_5vVTBQTBtcjJGzdChfPjtousslxAW0vLMFeApwliN-UOW4DhJ9p5O2Zt4KSk8ttKMKimQ0viCHkiTDhpNOeItthMobvLUObuOWoa8M4v82YceXzfMD-j07zEohqoY6YbtlT5nwxrrRFzoG4k30MnFriAFjY_B-zcJ8EWUnj5dY95N6W7CaDTUatIyDY5iYf9tIOJjnr_M65pdRDy9tzTdB4u-zpeHwRIW979ChuYuZytSo9iukae9ehj-UxCq-wouZZrsh5MDD-6DghLf7vUxwVmcfUmPATQPVq_6FABFvCndLrmg0NcNOU6w3U_8tMpR4MUwyEccPMGACnuhCZY8HUvmQaQvEyiVh2_n1sK-xWZvv4FXvJ009ntixrsJJu2ohOg4X7j6tmO9jNqsoL2csImwWnjggvrAuc6UAyciUIIKbMSpyUU1WN7Ht1sb1K0UtHGDZdxL4wzYNnW5xdYQJTvfs6P-kGds/download [1115393/1115393] -> \"input.txt\" [1]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print read_data[0:100]\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOO2MUdEbNuY"
   },
   "source": [
    "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11227,
     "status": "ok",
     "timestamp": 1561312272666,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "bBsrCGq_bNuZ",
    "outputId": "9e2f660d-24b0-4b75-f567-82116f3c81bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u\"'\", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [19  4 14 ... 14  9 20]\n",
      " [ 8 20 10 ...  8 10 18]\n",
      " ...\n",
      " [21  2  0 ...  0 21  0]\n",
      " [ 9  7  7 ...  0  2  3]\n",
      " [ 3  7  0 ...  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print \"vocabulary size:\" ,data_loader.vocab_size\n",
    "print \"Characters:\" ,data_loader.chars\n",
    "print \"vocab number of 'F':\",data_loader.vocab['F']\n",
    "print \"Character sequences (first batch):\", data_loader.x_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZngrIQFbNub"
   },
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11143,
     "status": "ok",
     "timestamp": 1561312272668,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "tqvVzqvDbNuc",
    "outputId": "1cb3c7c9-6422-4046-f357-5c8c8d9e7444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11465,
     "status": "ok",
     "timestamp": 1561312273074,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "LYSD2grBbNue",
    "outputId": "e985b344-48d5-4c3d-a9a4-d83eed063c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =60, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbTPCU5vbNui"
   },
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11384,
     "status": "ok",
     "timestamp": 1561312273076,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "rhmI98PkbNuj",
    "outputId": "0eb76f9b-694f-4f21-d7b3-dd66aa6e10a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ...,\n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mfGeY9FbNuo"
   },
   "source": [
    "### LSTM Architecture\n",
    "Each LSTM cell has 5 parts:\n",
    "1. Input\n",
    "2. prv_state\n",
    "3. prv_output\n",
    "4. new_state\n",
    "5. new_output\n",
    "\n",
    "\n",
    "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN, is defined by num_layers\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4GStvvHbNup"
   },
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "lMHoAW3abNuq"
   },
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13495,
     "status": "ok",
     "timestamp": 1561312275238,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "WJMSYRYLbNur",
    "outputId": "1bcdecfc-011d-42ac-d642-fff3892c5f69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 17:51:16.952330 140690090796928 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0623 17:51:16.953675 140690090796928 deprecation.py:323] From <ipython-input-10-cbc7d8d66937>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13481,
     "status": "ok",
     "timestamp": 1561312275240,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "wChFLvsHbNut",
    "outputId": "906e6a26-7cc2-40aa-df25-0f71f1d7f711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 17:51:16.970074 140690090796928 deprecation.py:323] From <ipython-input-11-32025279f672>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0623 17:51:16.977530 140690090796928 rnn_cell_impl.py:1642] At least two cells provided to MultiRNNCell are the same object and will share weights.\n"
     ]
    }
   ],
   "source": [
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13466,
     "status": "ok",
     "timestamp": 1561312275241,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "B8nlpbtYbNuv",
    "outputId": "82824593-cd51-4831-c9af-4c9ddcac6285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6mVeEigbNux"
   },
   "source": [
    "__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13456,
     "status": "ok",
     "timestamp": 1561312275242,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "7EBNSEK6bNuy",
    "outputId": "3a152043-013f-4bdd-aa22-24fa5b24aeac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EllpfaTybNu0"
   },
   "source": [
    "Lets define input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13451,
     "status": "ok",
     "timestamp": 1561312275248,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "K-YsOIBebNu1",
    "outputId": "c63c048b-9df2-428e-a802-9fdbd4828211"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp5_pFi1bNu4"
   },
   "source": [
    "and target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13437,
     "status": "ok",
     "timestamp": 1561312275249,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "aMZ3xkAsbNu6",
    "outputId": "8d0e20f6-bc3c-4420-8414-f0790a81fd38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guUXXmU6bNvA"
   },
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZSR7NB-bNvB"
   },
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2fcHIefebNvG"
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14460,
     "status": "ok",
     "timestamp": 1561312276293,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "oh59vOWMbNvJ",
    "outputId": "ec93e451-8165-4ade-9962-5835098a0459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcLkGGUSbNvO"
   },
   "source": [
    "### Embedding\n",
    "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBcn2cQ4bNvP"
   },
   "source": [
    "__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14716,
     "status": "ok",
     "timestamp": 1561312276567,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "kDAZCHFcbNvQ",
    "outputId": "b0b7a022-2ce1-4f07-f6c0-d55b3f08554d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 17:51:18.438700 140690090796928 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mC8M0IobNvT"
   },
   "source": [
    "Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16392,
     "status": "ok",
     "timestamp": 1561312278255,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "OhKRvPNQbNvT",
    "outputId": "fd6d6463-3311-4dec-8da7-7cc50b1d26ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4262426e-01,  6.7887023e-02,  5.5196181e-02, ...,\n",
       "         7.3216572e-02, -1.4559975e-01, -1.4511254e-01],\n",
       "       [-8.3636627e-02, -1.3837969e-01,  1.5285386e-01, ...,\n",
       "        -1.0404210e-01,  6.7058504e-02, -2.5729522e-02],\n",
       "       [ 5.1168352e-02, -1.4293146e-01, -4.0026322e-02, ...,\n",
       "         4.6390623e-02,  3.9161101e-02, -5.4033220e-02],\n",
       "       ...,\n",
       "       [-3.4817651e-02,  8.2675263e-02,  7.1360365e-02, ...,\n",
       "         1.7152055e-01,  1.1735891e-01, -1.3771653e-04],\n",
       "       [ 3.6253989e-02,  4.5640513e-02,  1.4998721e-01, ...,\n",
       "        -9.8379105e-03,  1.4838420e-01, -1.7284456e-01],\n",
       "       [-7.8081869e-02, -4.9438730e-02, -4.5006245e-02, ...,\n",
       "         5.1135689e-02, -1.4357567e-03, -1.5142187e-01]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdQutxeRbNvi"
   },
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16381,
     "status": "ok",
     "timestamp": 1561312278258,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "GB_hgyUnbNvj",
    "outputId": "ddf03480-9d1c-4012-af5a-28466125b531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.11922751,  0.01602827,  0.05483887, ..., -0.0088885 ,\n",
       "        -0.14458871, -0.01948926],\n",
       "       [-0.10270219, -0.06686227, -0.09569042, ...,  0.09198625,\n",
       "         0.06296253, -0.01269533],\n",
       "       [ 0.10339074, -0.07306953, -0.0215058 , ..., -0.08866797,\n",
       "        -0.07726311, -0.16640538],\n",
       "       ...,\n",
       "       [-0.08363663, -0.1383797 ,  0.15285386, ..., -0.1040421 ,\n",
       "         0.0670585 , -0.02572952],\n",
       "       [ 0.06525463,  0.05723324, -0.14590746, ..., -0.1565409 ,\n",
       "        -0.06636416,  0.01844716],\n",
       "       [ 0.10339074, -0.07306953, -0.0215058 , ..., -0.08866797,\n",
       "        -0.07726311, -0.16640538]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print emp.shape\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lslWRjGSbNvn"
   },
   "source": [
    "Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16372,
     "status": "ok",
     "timestamp": 1561312278259,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "wsDXNh67bNvo",
    "outputId": "b1179b0d-b6cf-4880-b0f0-b2b8440fce56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBPV3tLubNvq"
   },
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "\n",
    "The feeding process for iputs is as following:\n",
    "\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16366,
     "status": "ok",
     "timestamp": 1561312278261,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "zSUJElNobNvq",
    "outputId": "8667b0a5-f885-4bff-8a01-3488ef288c88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11922751,  0.01602827,  0.05483887, ..., -0.0088885 ,\n",
       "        -0.14458871, -0.01948926],\n",
       "       [ 0.00048381,  0.05899751,  0.12101318, ...,  0.0440184 ,\n",
       "        -0.05995357, -0.12071199],\n",
       "       [ 0.0029971 , -0.10345303, -0.02419911, ...,  0.00353822,\n",
       "         0.13114654,  0.11990021],\n",
       "       ...,\n",
       "       [-0.13110498,  0.13284327,  0.00771444, ..., -0.15886857,\n",
       "        -0.11516792,  0.00666794],\n",
       "       [-0.10270219, -0.06686227, -0.09569042, ...,  0.09198625,\n",
       "         0.06296253, -0.01269533],\n",
       "       [ 0.08677931, -0.11106488, -0.01058067, ..., -0.07863677,\n",
       "        -0.12912399,  0.143097  ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jperu_KrbNvt"
   },
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17353,
     "status": "ok",
     "timestamp": 1561312279264,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "cUmWamqlbNvu",
    "outputId": "604d5c77-a3f8-4463-8858-86109b59af3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 17:51:20.700969 140690090796928 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17350,
     "status": "ok",
     "timestamp": 1561312279276,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "1YHOqLqbbNvx",
    "outputId": "bab3a0f8-3b5c-44b1-b64f-ba760412944e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1hujbjXbNv0"
   },
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18296,
     "status": "ok",
     "timestamp": 1561312280231,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "ttkJiIbNbNv1",
    "outputId": "37b969b9-88a1-473e-9928-dca55ee9d127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17346424, -0.14763622,  0.05201026, ..., -0.0269255 ,\n",
       "         0.07404716,  0.12622212],\n",
       "       [ 0.07112692, -0.01390466, -0.03415237, ..., -0.04773943,\n",
       "         0.00474094,  0.08674454],\n",
       "       [ 0.06305302,  0.16891293, -0.03106111, ...,  0.01395927,\n",
       "        -0.04442436, -0.04344208],\n",
       "       ...,\n",
       "       [ 0.03023866,  0.06604872,  0.0350916 , ...,  0.04876231,\n",
       "         0.06610806,  0.01308044],\n",
       "       [-0.01913293,  0.02449315, -0.00867123, ...,  0.05243537,\n",
       "         0.07280112, -0.04343766],\n",
       "       [-0.01219481,  0.03029716, -0.03412472, ...,  0.11763198,\n",
       "        -0.00285417, -0.04956327]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEkWItLtbNv4"
   },
   "source": [
    "As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvW7JjbibNv5"
   },
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18289,
     "status": "ok",
     "timestamp": 1561312280233,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "Yz1NlE-ZbNv7",
    "outputId": "ced7e519-d4e6-4b24-87dd-a1a0b2df483d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18280,
     "status": "ok",
     "timestamp": 1561312280234,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "rDZRqOfSbNv_",
    "outputId": "139ceed5-b34b-40e7-c433-169fce4d3671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18271,
     "status": "ok",
     "timestamp": 1561312280235,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "n9KRHdLybNwB",
    "outputId": "bb89734d-aa88-4dab-e097-e7704ee46497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wg3IuxhCbNwE"
   },
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18257,
     "status": "ok",
     "timestamp": 1561312280236,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "2t_ASQYDbNwF",
    "outputId": "19ff39ca-8507-461b-e728-e6aa6962dc03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01400059, 0.01184102, 0.01767516, ..., 0.01456283, 0.01976584,\n",
       "        0.01659797],\n",
       "       [0.01493088, 0.01181441, 0.01702588, ..., 0.01388809, 0.01839338,\n",
       "        0.01778731],\n",
       "       [0.01489643, 0.01305431, 0.01468731, ..., 0.01272967, 0.02086691,\n",
       "        0.01398379],\n",
       "       ...,\n",
       "       [0.01565715, 0.01189745, 0.02355565, ..., 0.01001661, 0.01474348,\n",
       "        0.01741084],\n",
       "       [0.01214073, 0.01332532, 0.0207196 , ..., 0.01508496, 0.01486792,\n",
       "        0.01721523],\n",
       "       [0.01496346, 0.01598558, 0.01485762, ..., 0.01235383, 0.01258977,\n",
       "        0.01263217]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OofODRZbNwK"
   },
   "source": [
    "Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18247,
     "status": "ok",
     "timestamp": 1561312280238,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "WGILTShubNwK",
    "outputId": "ed49e1e0-5417-446c-8868-e293fdfae2f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry7l5xWzbNwM"
   },
   "source": [
    "# All together\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zs9_kix4bNwM"
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "ZKzlRmjvbNwP"
   },
   "source": [
    "### Creating the LSTM object\n",
    "Now we create a LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20483,
     "status": "ok",
     "timestamp": 1561312282494,
     "user": {
      "displayName": "Dennis Aprilla Christie",
      "photoUrl": "https://lh3.googleusercontent.com/-W_6VdV2VWBM/AAAAAAAAAAI/AAAAAAAADr0/Jovfu3kCHdA/s64/photo.jpg",
      "userId": "09637515618974467169"
     },
     "user_tz": -420
    },
    "id": "NFOoC70ZbNwQ",
    "outputId": "f3a73611-edc5-4a6b-9c7b-eeae89ddc6bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 17:51:24.306227 140690090796928 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/clip_ops.py:286: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEdW5t7qbNwT"
   },
   "source": [
    "# Train usinng LSTMModel class\n",
    "We can train our model through feeding batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1360
    },
    "colab_type": "code",
    "id": "3fHHML6sbNwV",
    "outputId": "70c8902c-9bef-4c82-8b5c-3efe4238b107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/46375 (epoch 0), train_loss = 1.937, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The Cash to porder'd of chas oneting untes igl therelf\n",
      "----------------------------------\n",
      "741/46375 (epoch 1), train_loss = 1.767, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The chauce, my look, a pare I kelle?\n",
      "\n",
      "SIUSO The griebs\n",
      "----------------------------------\n",
      "1112/46375 (epoch 2), train_loss = 1.688, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The frow am:\n",
      "Though comfort of me,\n",
      "And both I yef test\n",
      "----------------------------------\n",
      "1483/46375 (epoch 3), train_loss = 1.638, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The great dome to gaid,\n",
      "Which spiercio\n",
      "shall his\n",
      "are e\n",
      "----------------------------------\n",
      "1854/46375 (epoch 4), train_loss = 1.607, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The tibne is no, live your sow trumbons\n",
      "Honreed, the c\n",
      "----------------------------------\n",
      "2225/46375 (epoch 5), train_loss = 1.586, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The string brealies\n",
      "razas;' murder the bound and upon \n",
      "----------------------------------\n",
      "2596/46375 (epoch 6), train_loss = 1.572, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The ruture a tooch!\n",
      "Ay, fooletneef's fior offer a\n",
      "For \n",
      "----------------------------------\n",
      "2967/46375 (epoch 7), train_loss = 1.559, time/batch = 0.013\n",
      ">> sample mode:\n",
      "The of nunt we miness,\n",
      "Lickated bad at I love entersed\n",
      "----------------------------------\n",
      "3338/46375 (epoch 8), train_loss = 1.548, time/batch = 0.013\n",
      ">> sample mode:\n",
      "The vown but father to be reteping shild\n",
      "to unk you sp\n",
      "----------------------------------\n",
      "3709/46375 (epoch 9), train_loss = 1.539, time/batch = 0.013\n",
      ">> sample mode:\n",
      "The that's that indeed! whose love sunced to yer'd wil\n",
      "----------------------------------\n",
      "4080/46375 (epoch 10), train_loss = 1.531, time/batch = 0.016\n",
      ">> sample mode:\n",
      "The lionce\n",
      "And not sinks londs batch'd thrick'd\n",
      "silkor\n",
      "----------------------------------\n",
      "4451/46375 (epoch 11), train_loss = 1.523, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The shall for us.\n",
      "\n",
      "BAPTISTA:\n",
      "He seem her quest;\n",
      "My sou\n",
      "----------------------------------\n",
      "4822/46375 (epoch 12), train_loss = 1.516, time/batch = 0.013\n",
      ">> sample mode:\n",
      "The this grace, oncreplently to. I set you,\n",
      "Cal Gramit\n",
      "----------------------------------\n",
      "5193/46375 (epoch 13), train_loss = 1.509, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The save the kisher is we round wish\n",
      "Cure.\n",
      "Condicion\n",
      "a\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1)\n",
    "            print '----------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "lYiw-iu_bNwY"
   },
   "source": [
    "# Thanks for completing this lesson!\n",
    "Created by: <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eyt5VMuIcLho"
   },
   "source": [
    "<hr>\n",
    "\n",
    "<p>Copyright &copy; 2017 IBM <a href=\"https://cognitiveclass.ai/?utm_source=ML0151&utm_medium=lab&utm_campaign=cclab\">IBM Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://cognitiveclass.ai/mit-license/\">MIT License</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ML0120EN-3.4-Review-LSTM-CharacterModelling.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
